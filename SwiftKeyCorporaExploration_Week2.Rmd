---
title: "SwiftKey Training Data Exploration"
author: "Aaron Trask"
date: "October 25, 2017"
output:
  html_document: default
  html_notebook: default
---

## Project Overview
The goal of this project is to predict the next word in a sequence using an ngram model.  The training data cleaned and explored here is free text extracted from blogs, news articles, and twitter.  Tidytext and other supporting packages are used to clean and explore the data.

```{r, message=FALSE}
library(dplyr)
library(tidytext)
library(stringr)
library(ggplot2)
library(hunspell)
library(tidyr)
library(igraph)
library(ggraph)
```

## Data Preparation
We will be working with the english language files in the dataset.
```{r}
en_files <- Sys.glob("final/en_US/*.txt")
file.info(en_files)
```

Since we will be create n-grams later which will increase the memory needed to store them, we need to create a smaller subset of these files.
```{r}
set.seed(1234)

# loop over files and save sample
for (i in 1:length(en_files)) {
  # create some meta data from the file name
  explodePath <- unlist(strsplit(en_files[i], "[.]|[/]"))
  
  # read in the file to a data frame with rows for each line
  con <- file(en_files[i], open="r")
  allLines <- readLines(con)
  close(con)
  
  # write the sample to a file
  sample_prob <- 0.05 # look at about 5% of the lines in the files
  sampledLines <- allLines[rbinom(n = length(allLines), size = 1, prob = sample_prob)==1]
  write.csv(sampledLines, file = paste(explodePath[3],explodePath[4],"sampled",explodePath[5],sep = "."))
}

sample_en_files <- Sys.glob("en_US.*.sampled.txt")
file.info(sample_en_files)
```

## Word Frequency
The samples still contain *** messy *** text which we need to tidy and filter for words with punctuation, special characters, numbers, profanity, and non-english dictionary words.
```{r}
# create empty data frame
corpora_words <- data.frame(words=character(),
                            n=integer(),
                            stringsAsFactors = FALSE)

# loop over files and convert to corpora
for (i in 1:length(sample_en_files)) {
  # create some meta data from the file name
  explodePath <- unlist(strsplit(sample_en_files[i], "[.]"))
  
  # read in the file to a data frame with rows for each line
  con <- file(sample_en_files[i], open="r")
  allLines <- readLines(con)
  close(con)

  # tokenize into words
  words <- data.frame(txt_lines=allLines, stringsAsFactors = FALSE) %>% 
    unnest_tokens(word, txt_lines)

  # append data frame
  corpora_words <- rbind(corpora_words, words)
}

# add word count
corpora_words <- corpora_words %>%
    count(word, sort = TRUE) %>%
    ungroup()

# read in profanity word list from Shutterstock project located at
# https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
con <- file("../List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/en", open="r")
profanity <- readLines(con)
close(con)

# filter out profanity, non-alphabetic characters, 3 or more repeated characters, and non-english words
corpora_filtered <- corpora_words %>% 
  filter(!word %in% profanity) %>% # only keep words that aren't profanity
  filter(str_detect(word, "^[:alpha:]+$")) %>% # only keep words with alphabetic characters
  filter(!str_detect(word, "([:alpha:])\\1{2,}")) %>% # remove words with 3 or more repeated characters
  filter(hunspell_check(word, dict = "en_US")) # remove words not in the english dictionary

summary(corpora_filtered)
```

The top most frequent words (below) are stop words.  Typically these are removed in other types of natural language processing, but for sequence prediction, they will need to be included in the model.
```{r}
corpora_filtered %>%
  arrange(desc(n)) %>%
  head(10)
```

Now we can look at the distribution of word occurance.  Dividing the individual word count by the document total gives us a relative comparison.
```{r}
# calculate word count in each file
total_words <- corpora_filtered %>%
  summarize(total = sum(n))

# add word counts to data frame
corpora_filtered$freq <- corpora_filtered$n/total_words$total

# histogram of word frequency
ggplot(corpora_filtered, aes(freq)) +
  geom_histogram(show.legend = FALSE, binwidth = 1e-06) +
  xlim(NA, 0.00005)
```

## Bigram and Trigram Frequency
The following bigram and trigram samples use the same process above, but with 2 and 3 word tokens instead of single words.
```{r}
# create empty data frame
corpora_bigrams <- data.frame(bigram=character(), n=integer(), stringsAsFactors = FALSE)
corpora_trigrams <- data.frame(trigram=character(), n=integer(), stringsAsFactors = FALSE)

# loop over files and convert to corpora
for (i in 1:length(sample_en_files)) {
  # read in the file to a data frame with rows for each line
  con <- file(sample_en_files[i], open="r")
  allLines <- readLines(con)
  close(con)

  # tokenize and group bigrams to get frequency
  bigrams <- data.frame(txt_lines=allLines, stringsAsFactors = FALSE) %>%
    unnest_tokens(bigram, txt_lines, token = "ngrams", n = 2)
  
  # tokenize and group trigrams to get frequency
  trigrams <- data.frame(txt_lines=allLines, stringsAsFactors = FALSE) %>%
    unnest_tokens(trigram, txt_lines, token = "ngrams", n = 3)

  # append data frame
  corpora_bigrams <- rbind(corpora_bigrams, bigrams)
  corpora_trigrams <- rbind(corpora_trigrams, trigrams)
}

# separate and count
corpora_bigrams <- corpora_bigrams %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    count(word1, word2, sort = TRUE) %>%
    ungroup()
corpora_trigrams <- corpora_trigrams %>%
    separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
    count(word1, word2, word3, sort = TRUE) %>%
    ungroup()

# read in profanity word list from Shutterstock project located at
# https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
con <- file("../List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/en", open="r")
profanity <- readLines(con)
close(con)

# filter out profanity, non-alphabetic characters, 3 or more repeated characters, and non-english words
corpora_bigrams_filtered <- corpora_bigrams %>% 
  filter(!word1 %in% profanity | !word2 %in% profanity) %>% # only keep words that aren't profanity
  filter(str_detect(word1, "^[:alpha:]+$") & str_detect(word2, "^[:alpha:]+$")) %>% # only keep words with alphabetic characters
  filter(!str_detect(word1, "([:alpha:])\\1{2,}") | !str_detect(word2, "([:alpha:])\\1{2,}")) %>% # remove words with 3 or more repeated characters
  filter(hunspell_check(word1, dict = "en_US") & hunspell_check(word2, dict = "en_US")) # remove words not in the english dictionary

corpora_trigrams_filtered <- corpora_trigrams %>% 
  filter(!word1 %in% profanity | !word2 %in% profanity | !word3 %in% profanity) %>% # only keep words that aren't profanity
  filter(str_detect(word1, "^[:alpha:]+$") & str_detect(word2, "^[:alpha:]+$") & str_detect(word3, "^[:alpha:]+$")) %>% # only keep words with alphabetic characters
  filter(!str_detect(word1, "([:alpha:])\\1{2,}") | !str_detect(word2, "([:alpha:])\\1{2,}") | !str_detect(word3, "([:alpha:])\\1{2,}")) %>% # remove words with 3 or more repeated characters
  filter(hunspell_check(word1, dict = "en_US") & hunspell_check(word2, dict = "en_US") & hunspell_check(word3, dict = "en_US")) # remove words not in the english dictionary
```

The most frequent bigrams in the sample are:
```{r, echo=FALSE}
head(corpora_bigrams_filtered)
```

The most frequent trigrams in the sample are:
```{r, echo=FALSE}
head(corpora_trigrams_filtered)
```

## Model Storage Considerations
We can also draw a graph from the bigrams to get an idea of how words are related.  Looking at graph storage and query might be a way to optimize the n-gram model later on.  This graph is of the bigrams with more than 500 occurances.
```{r}
# filter for common combinations
bigram_graph <- corpora_bigrams_filtered %>%
  filter(n > 500) %>%
  graph_from_data_frame()

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


## Future Model Work
The model will follow the above methods to produce 2, 3, and 4 word ngrams for use in predicting the next word in a sequence.  Since the dataset is large and is to big to be completly included in the model, some trade offs will need to be made.

Trades:
- model size
- model runtime
- model accuracy